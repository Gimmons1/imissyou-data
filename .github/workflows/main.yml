import requests
import feedparser
import json
import os
from datetime import datetime

# --- CONFIGURAZIONE ---
# Fonti affidabili certificate per il monitoraggio
FEEDS = [
    "https://www.ansa.it/sito/notizie/cultura/cultura_rss.xml",
    "https://feeds.bbci.co.uk/news/world/rss.xml",
    "https://www.reutersagency.com/feed/"
]

# Parole chiave per intercettare i decessi nei titoli
KEYWORDS = ["morto", "morta", "deceduto", "deceduta", "scomparsa", "scomparso", "addio a", "died", "passed away", "death of"]
JSON_FILE = "library.json"

def get_wikipedia_details(name, lang="it"):
    """Cerca dettagli profondi (causa morte, bio) su Wikipedia tramite API"""
    url = f"https://{lang}.wikipedia.org/api/rest_v1/page/summary/{name.replace(' ', '_')}"
    try:
        response = requests.get(url, timeout=10)
        if response.status_code == 200:
            data = response.json()
            return {
                "slug": data.get("titles", {}).get("canonical", ""),
                "bio": data.get("extract", ""),
                "img": data.get("originalimage", {}).get("source", None)
            }
    except Exception as e:
        print(f"Errore nel recupero dati Wikipedia per {name}: {e}")
    return None

def run_updater():
    print(f"--- Inizio scansione: {datetime.now()} ---")
    
    # 1. Carica il database esistente per evitare duplicati
    if os.path.exists(JSON_FILE):
        with open(JSON_FILE, "r", encoding="utf-8") as f:
            try:
                library = json.load(f)
            except:
                library = []
    else:
        library = []

    existing_names = [p["name"].lower() for p in library]
    new_entries = []

    # 2. Scansiona ogni feed RSS
    for feed_url in FEEDS:
        print(f"Controllo in corso: {feed_url}")
        feed = feedparser.parse(feed_url)
        for entry in feed.entries:
            title = entry.title.lower()
            
            # 3. Se trova una parola chiave, prova a estrarre il nome
            if any(key in title for key in KEYWORDS):
                # Pulizia base del nome dal titolo della notizia
                potential_name = entry.title.replace("Addio a ", "").replace("Morto ", "").replace("È morto ", "").replace("Morta ", "").split(",")[0].strip()
                
                if potential_name.lower() not in existing_names:
                    print(f"!!! Trovato possibile decesso: {potential_name}")
                    
                    # Recupera le info ufficiali da Wikipedia
                    info = get_wikipedia_details(potential_name)
                    if info and info["slug"]:
                        new_person = {
                            "name": potential_name,
                            "slugs": {"IT": info["slug"], "EN": info["slug"]},
                            "bio": info["bio"] + " [BOZZA: Verifica dettagli e causa morte]",
                            "birthDate": "1900-01-01", # Da correggere manualmente durante l'approvazione
                            "deathDate": datetime.now().strftime("%Y-%m-%d"),
                            "imageUrl": info["img"]
                        }
                        new_entries.append(new_person)
                        existing_names.append(potential_name.lower())

    # 4. Aggiorna il file JSON locale (che poi GitHub Actions invierà per approvazione)
    if new_entries:
        library.extend(new_entries)
        with open(JSON_FILE, "w", encoding="utf-8") as f:
            json.dump(library, f, indent=2, ensure_ascii=False)
        print(f"Aggiunti {len(new_entries)} decessi in bozza.")
    else:
        print("Nessun nuovo decesso rilevato.")

if __name__ == "__main__":
    run_updater()
